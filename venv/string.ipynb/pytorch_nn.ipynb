{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#gradient calculation single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "#form ==> x-->square-->y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(3,requires_grad=True,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward propagation\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of y\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#gradient calculation using nested function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "#form ==> x-->square-->y-->sin-->z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(3,requires_grad=True,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=torch.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4121, grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.4668)"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch is actually creating a one graph in the backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a simple neural network from scratch using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Neural Network will be five inputs one hidden layer and one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model1(nn.Module):\n",
    "    def __init__(self,new_features):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(new_features,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,features):\n",
    "        out=self.linear(features)\n",
    "        out=self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=torch.rand(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9635, 0.3533, 0.4746, 0.8527, 0.5379],\n",
       "        [0.3864, 0.5826, 0.1110, 0.3183, 0.1051],\n",
       "        [0.0446, 0.1263, 0.9793, 0.5490, 0.4526],\n",
       "        [0.7495, 0.9479, 0.9178, 0.0822, 0.6906],\n",
       "        [0.4149, 0.2414, 0.0512, 0.9834, 0.2900],\n",
       "        [0.2661, 0.3643, 0.6151, 0.9263, 0.1971],\n",
       "        [0.9747, 0.5037, 0.0171, 0.3135, 0.1477],\n",
       "        [0.1613, 0.2905, 0.8277, 0.1789, 0.6187],\n",
       "        [0.4641, 0.8895, 0.2785, 0.1648, 0.4379],\n",
       "        [0.1051, 0.9250, 0.2741, 0.3871, 0.1508]])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=model1(data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3743],\n",
       "        [0.4123],\n",
       "        [0.3563],\n",
       "        [0.2940],\n",
       "        [0.4162],\n",
       "        [0.3810],\n",
       "        [0.4369],\n",
       "        [0.3425],\n",
       "        [0.3463],\n",
       "        [0.3594]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3743],\n",
       "        [0.4123],\n",
       "        [0.3563],\n",
       "        [0.2940],\n",
       "        [0.4162],\n",
       "        [0.3810],\n",
       "        [0.4369],\n",
       "        [0.3425],\n",
       "        [0.3463],\n",
       "        [0.3594]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1160, -0.3751, -0.2284, -0.0705, -0.4423]], requires_grad=True)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0865], requires_grad=True)"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Neural Network will five input three hidden layer and one output\n",
    "\n",
    "#hidden layer -->reu \n",
    "\n",
    "#output layer -->sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model2(nn.Module):\n",
    "    def __init__(self,new_features):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(new_features,3)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(3,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,features):\n",
    "        out=self.linear1(features)\n",
    "        out=self.relu(out)\n",
    "        out=self.linear2(out)\n",
    "        out=self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=torch.rand(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1=model2(data1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model2(\n",
       "  (linear1): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5706],\n",
       "        [0.5634],\n",
       "        [0.5635],\n",
       "        [0.5733],\n",
       "        [0.5736],\n",
       "        [0.5634],\n",
       "        [0.5669],\n",
       "        [0.5683],\n",
       "        [0.5573],\n",
       "        [0.5598]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj1(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5706],\n",
       "        [0.5634],\n",
       "        [0.5635],\n",
       "        [0.5733],\n",
       "        [0.5736],\n",
       "        [0.5634],\n",
       "        [0.5669],\n",
       "        [0.5683],\n",
       "        [0.5573],\n",
       "        [0.5598]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj1.forward(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creation of inbuilt method  using the Sequential method in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class custom(nn.Module):\n",
    "    def __init__(self,new_features):\n",
    "        super().__init__()\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(new_features,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self,features):\n",
    "        out=self.network(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3667, 0.7574, 0.4872, 0.6693, 0.3801],\n",
       "        [0.9643, 0.5469, 0.5784, 0.2485, 0.1483],\n",
       "        [0.2012, 0.8172, 0.3828, 0.2497, 0.1068],\n",
       "        [0.5691, 0.3763, 0.0508, 0.6898, 0.5635],\n",
       "        [0.7962, 0.1707, 0.7528, 0.4973, 0.1660],\n",
       "        [0.5759, 0.6345, 0.3523, 0.4693, 0.6162],\n",
       "        [0.6384, 0.9715, 0.1525, 0.8013, 0.2142],\n",
       "        [0.4036, 0.8086, 0.6568, 0.6012, 0.8208],\n",
       "        [0.1806, 0.8593, 0.5284, 0.0467, 0.7799],\n",
       "        [0.5616, 0.7185, 0.4547, 0.1144, 0.2351]])"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2=custom(data1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7371],\n",
       "        [0.7107],\n",
       "        [0.7124],\n",
       "        [0.7010],\n",
       "        [0.7128],\n",
       "        [0.7182],\n",
       "        [0.7364],\n",
       "        [0.7480],\n",
       "        [0.7182],\n",
       "        [0.7065]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj2(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7371],\n",
       "        [0.7107],\n",
       "        [0.7124],\n",
       "        [0.7010],\n",
       "        [0.7128],\n",
       "        [0.7182],\n",
       "        [0.7364],\n",
       "        [0.7480],\n",
       "        [0.7182],\n",
       "        [0.7065]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj2.forward(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#using dataset  to perform the model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 569\\n\\n:Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n:Attribute Information:\\n    - radius (mean of distances from center to points on the perimeter)\\n    - texture (standard deviation of gray-scale values)\\n    - perimeter\\n    - area\\n    - smoothness (local variation in radius lengths)\\n    - compactness (perimeter^2 / area - 1.0)\\n    - concavity (severity of concave portions of the contour)\\n    - concave points (number of concave portions of the contour)\\n    - symmetry\\n    - fractal dimension (\"coastline approximation\" - 1)\\n\\n    The mean, standard error, and \"worst\" or largest (mean of the three\\n    worst/largest values) of these features were computed for each image,\\n    resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n    10 is Radius SE, field 20 is Worst Radius.\\n\\n    - class:\\n            - WDBC-Malignant\\n            - WDBC-Benign\\n\\n:Summary Statistics:\\n\\n===================================== ====== ======\\n                                        Min    Max\\n===================================== ====== ======\\nradius (mean):                        6.981  28.11\\ntexture (mean):                       9.71   39.28\\nperimeter (mean):                     43.79  188.5\\narea (mean):                          143.5  2501.0\\nsmoothness (mean):                    0.053  0.163\\ncompactness (mean):                   0.019  0.345\\nconcavity (mean):                     0.0    0.427\\nconcave points (mean):                0.0    0.201\\nsymmetry (mean):                      0.106  0.304\\nfractal dimension (mean):             0.05   0.097\\nradius (standard error):              0.112  2.873\\ntexture (standard error):             0.36   4.885\\nperimeter (standard error):           0.757  21.98\\narea (standard error):                6.802  542.2\\nsmoothness (standard error):          0.002  0.031\\ncompactness (standard error):         0.002  0.135\\nconcavity (standard error):           0.0    0.396\\nconcave points (standard error):      0.0    0.053\\nsymmetry (standard error):            0.008  0.079\\nfractal dimension (standard error):   0.001  0.03\\nradius (worst):                       7.93   36.04\\ntexture (worst):                      12.02  49.54\\nperimeter (worst):                    50.41  251.2\\narea (worst):                         185.2  4254.0\\nsmoothness (worst):                   0.071  0.223\\ncompactness (worst):                  0.027  1.058\\nconcavity (worst):                    0.0    1.252\\nconcave points (worst):               0.0    0.291\\nsymmetry (worst):                     0.156  0.664\\nfractal dimension (worst):            0.055  0.208\\n===================================== ====== ======\\n\\n:Missing Attribute Values: None\\n\\n:Class Distribution: 212 - Malignant, 357 - Benign\\n\\n:Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n:Donor: Nick Street\\n\\n:Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. dropdown:: References\\n\\n  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\\n    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\\n    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n    San Jose, CA, 1993.\\n  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\\n    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\\n    July-August 1995.\\n  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\\n    163-171.\\n',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf=pd.DataFrame(data=data2.data,columns=data2.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf[\"cancer\"]=data2.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  ...  worst fractal dimension  cancer\n",
       "0        17.99         10.38  ...                  0.11890       0\n",
       "1        20.57         17.77  ...                  0.08902       0\n",
       "2        19.69         21.25  ...                  0.08758       0\n",
       "3        11.42         20.38  ...                  0.17300       0\n",
       "4        20.29         14.34  ...                  0.07678       0\n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>1.256</td>\n",
       "      <td>7.673</td>\n",
       "      <td>158.70</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.02891</td>\n",
       "      <td>0.05198</td>\n",
       "      <td>0.02454</td>\n",
       "      <td>0.01114</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>2.463</td>\n",
       "      <td>5.203</td>\n",
       "      <td>99.04</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.02423</td>\n",
       "      <td>0.03950</td>\n",
       "      <td>0.01678</td>\n",
       "      <td>0.01898</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>1.075</td>\n",
       "      <td>3.425</td>\n",
       "      <td>48.55</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.03731</td>\n",
       "      <td>0.04730</td>\n",
       "      <td>0.01557</td>\n",
       "      <td>0.01318</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>0.7260</td>\n",
       "      <td>1.595</td>\n",
       "      <td>5.772</td>\n",
       "      <td>86.22</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.06158</td>\n",
       "      <td>0.07117</td>\n",
       "      <td>0.01664</td>\n",
       "      <td>0.02324</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>1.428</td>\n",
       "      <td>2.548</td>\n",
       "      <td>19.15</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.00466</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02676</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  ...  worst fractal dimension  cancer\n",
       "564        21.56         22.39  ...                  0.07115       0\n",
       "565        20.13         28.25  ...                  0.06637       0\n",
       "566        16.60         28.08  ...                  0.07820       0\n",
       "567        20.60         29.33  ...                  0.12400       0\n",
       "568         7.76         24.54  ...                  0.07039       1\n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
       "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
       "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
       "       'worst concave points', 'worst symmetry', 'worst fractal dimension',\n",
       "       'cancer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                0\n",
       "mean texture               0\n",
       "mean perimeter             0\n",
       "mean area                  0\n",
       "mean smoothness            0\n",
       "mean compactness           0\n",
       "mean concavity             0\n",
       "mean concave points        0\n",
       "mean symmetry              0\n",
       "mean fractal dimension     0\n",
       "radius error               0\n",
       "texture error              0\n",
       "perimeter error            0\n",
       "area error                 0\n",
       "smoothness error           0\n",
       "compactness error          0\n",
       "concavity error            0\n",
       "concave points error       0\n",
       "symmetry error             0\n",
       "fractal dimension error    0\n",
       "worst radius               0\n",
       "worst texture              0\n",
       "worst perimeter            0\n",
       "worst area                 0\n",
       "worst smoothness           0\n",
       "worst compactness          0\n",
       "worst concavity            0\n",
       "worst concave points       0\n",
       "worst symmetry             0\n",
       "worst fractal dimension    0\n",
       "cancer                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pf.iloc[:,0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>1.2560</td>\n",
       "      <td>7.673</td>\n",
       "      <td>158.70</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.02891</td>\n",
       "      <td>0.05198</td>\n",
       "      <td>0.02454</td>\n",
       "      <td>0.01114</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>2.4630</td>\n",
       "      <td>5.203</td>\n",
       "      <td>99.04</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.02423</td>\n",
       "      <td>0.03950</td>\n",
       "      <td>0.01678</td>\n",
       "      <td>0.01898</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>1.0750</td>\n",
       "      <td>3.425</td>\n",
       "      <td>48.55</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.03731</td>\n",
       "      <td>0.04730</td>\n",
       "      <td>0.01557</td>\n",
       "      <td>0.01318</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>0.7260</td>\n",
       "      <td>1.5950</td>\n",
       "      <td>5.772</td>\n",
       "      <td>86.22</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.06158</td>\n",
       "      <td>0.07117</td>\n",
       "      <td>0.01664</td>\n",
       "      <td>0.02324</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>1.4280</td>\n",
       "      <td>2.548</td>\n",
       "      <td>19.15</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.00466</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02676</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
       "0          17.99         10.38  ...          0.4601                  0.11890\n",
       "1          20.57         17.77  ...          0.2750                  0.08902\n",
       "2          19.69         21.25  ...          0.3613                  0.08758\n",
       "3          11.42         20.38  ...          0.6638                  0.17300\n",
       "4          20.29         14.34  ...          0.2364                  0.07678\n",
       "..           ...           ...  ...             ...                      ...\n",
       "564        21.56         22.39  ...          0.2060                  0.07115\n",
       "565        20.13         28.25  ...          0.2572                  0.06637\n",
       "566        16.60         28.08  ...          0.2218                  0.07820\n",
       "567        20.60         29.33  ...          0.4087                  0.12400\n",
       "568         7.76         24.54  ...          0.2871                  0.07039\n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pf.loc[:,['cancer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#0==>malignant cancer\n",
    "\n",
    "#1==>benign no cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=5,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30), (455, 1), (114, 1))"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>16.300</td>\n",
       "      <td>15.70</td>\n",
       "      <td>104.70</td>\n",
       "      <td>819.8</td>\n",
       "      <td>0.09427</td>\n",
       "      <td>0.06712</td>\n",
       "      <td>0.05526</td>\n",
       "      <td>0.04563</td>\n",
       "      <td>0.1711</td>\n",
       "      <td>0.05657</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.4706</td>\n",
       "      <td>1.146</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.007394</td>\n",
       "      <td>0.01203</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.01344</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>17.32</td>\n",
       "      <td>17.76</td>\n",
       "      <td>109.80</td>\n",
       "      <td>928.2</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.13610</td>\n",
       "      <td>0.19470</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.07230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>9.847</td>\n",
       "      <td>15.68</td>\n",
       "      <td>63.00</td>\n",
       "      <td>293.2</td>\n",
       "      <td>0.09492</td>\n",
       "      <td>0.08419</td>\n",
       "      <td>0.02330</td>\n",
       "      <td>0.02416</td>\n",
       "      <td>0.1387</td>\n",
       "      <td>0.06891</td>\n",
       "      <td>0.2498</td>\n",
       "      <td>1.2160</td>\n",
       "      <td>1.976</td>\n",
       "      <td>15.24</td>\n",
       "      <td>0.008732</td>\n",
       "      <td>0.02042</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.01824</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>11.24</td>\n",
       "      <td>22.99</td>\n",
       "      <td>74.32</td>\n",
       "      <td>376.5</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>0.22430</td>\n",
       "      <td>0.08434</td>\n",
       "      <td>0.06528</td>\n",
       "      <td>0.2502</td>\n",
       "      <td>0.09209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>19.190</td>\n",
       "      <td>15.94</td>\n",
       "      <td>126.30</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>0.08694</td>\n",
       "      <td>0.11850</td>\n",
       "      <td>0.11930</td>\n",
       "      <td>0.09667</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.05176</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6336</td>\n",
       "      <td>6.971</td>\n",
       "      <td>119.30</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>0.03055</td>\n",
       "      <td>0.043440</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>0.03156</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>22.03</td>\n",
       "      <td>17.81</td>\n",
       "      <td>146.60</td>\n",
       "      <td>1495.0</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>0.20160</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.17770</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>0.06251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>16.030</td>\n",
       "      <td>15.51</td>\n",
       "      <td>105.80</td>\n",
       "      <td>793.2</td>\n",
       "      <td>0.09491</td>\n",
       "      <td>0.13710</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.07041</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.05976</td>\n",
       "      <td>0.3371</td>\n",
       "      <td>0.7476</td>\n",
       "      <td>2.629</td>\n",
       "      <td>33.27</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>0.03245</td>\n",
       "      <td>0.037150</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>0.01467</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>18.76</td>\n",
       "      <td>21.98</td>\n",
       "      <td>124.30</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.1435</td>\n",
       "      <td>0.44780</td>\n",
       "      <td>0.49560</td>\n",
       "      <td>0.19810</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.09124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>10.710</td>\n",
       "      <td>20.39</td>\n",
       "      <td>69.50</td>\n",
       "      <td>344.9</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.12890</td>\n",
       "      <td>0.08448</td>\n",
       "      <td>0.02867</td>\n",
       "      <td>0.1668</td>\n",
       "      <td>0.06862</td>\n",
       "      <td>0.3198</td>\n",
       "      <td>1.4890</td>\n",
       "      <td>2.230</td>\n",
       "      <td>20.74</td>\n",
       "      <td>0.008902</td>\n",
       "      <td>0.04785</td>\n",
       "      <td>0.073390</td>\n",
       "      <td>0.017450</td>\n",
       "      <td>0.02728</td>\n",
       "      <td>0.007610</td>\n",
       "      <td>11.69</td>\n",
       "      <td>25.21</td>\n",
       "      <td>76.51</td>\n",
       "      <td>410.4</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.25340</td>\n",
       "      <td>0.08600</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>0.08701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>12.980</td>\n",
       "      <td>19.35</td>\n",
       "      <td>84.52</td>\n",
       "      <td>514.0</td>\n",
       "      <td>0.09579</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.07107</td>\n",
       "      <td>0.02950</td>\n",
       "      <td>0.1761</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>0.5664</td>\n",
       "      <td>2.465</td>\n",
       "      <td>20.65</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.03255</td>\n",
       "      <td>0.043930</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.02751</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>14.42</td>\n",
       "      <td>21.95</td>\n",
       "      <td>99.21</td>\n",
       "      <td>634.3</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>0.32530</td>\n",
       "      <td>0.34390</td>\n",
       "      <td>0.09858</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.09166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>12.720</td>\n",
       "      <td>17.67</td>\n",
       "      <td>80.98</td>\n",
       "      <td>501.3</td>\n",
       "      <td>0.07896</td>\n",
       "      <td>0.04522</td>\n",
       "      <td>0.01402</td>\n",
       "      <td>0.01835</td>\n",
       "      <td>0.1459</td>\n",
       "      <td>0.05544</td>\n",
       "      <td>0.2954</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>2.109</td>\n",
       "      <td>23.24</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>0.01174</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.01940</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>13.82</td>\n",
       "      <td>20.96</td>\n",
       "      <td>88.87</td>\n",
       "      <td>586.8</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.09605</td>\n",
       "      <td>0.03469</td>\n",
       "      <td>0.03612</td>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.06025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>16.460</td>\n",
       "      <td>20.11</td>\n",
       "      <td>109.30</td>\n",
       "      <td>832.9</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.17930</td>\n",
       "      <td>0.08866</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.06323</td>\n",
       "      <td>0.3037</td>\n",
       "      <td>1.2840</td>\n",
       "      <td>2.482</td>\n",
       "      <td>31.59</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.04094</td>\n",
       "      <td>0.053710</td>\n",
       "      <td>0.018130</td>\n",
       "      <td>0.01682</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>17.79</td>\n",
       "      <td>28.45</td>\n",
       "      <td>123.50</td>\n",
       "      <td>981.2</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.46670</td>\n",
       "      <td>0.58620</td>\n",
       "      <td>0.20350</td>\n",
       "      <td>0.3054</td>\n",
       "      <td>0.09519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>15.040</td>\n",
       "      <td>16.74</td>\n",
       "      <td>98.73</td>\n",
       "      <td>689.4</td>\n",
       "      <td>0.09883</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.06142</td>\n",
       "      <td>0.1668</td>\n",
       "      <td>0.06869</td>\n",
       "      <td>0.3720</td>\n",
       "      <td>0.8423</td>\n",
       "      <td>2.304</td>\n",
       "      <td>34.84</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.01819</td>\n",
       "      <td>0.019960</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.01055</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>16.76</td>\n",
       "      <td>20.43</td>\n",
       "      <td>109.70</td>\n",
       "      <td>856.9</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.21760</td>\n",
       "      <td>0.18560</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.2177</td>\n",
       "      <td>0.08549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>13.150</td>\n",
       "      <td>15.34</td>\n",
       "      <td>85.31</td>\n",
       "      <td>538.9</td>\n",
       "      <td>0.09384</td>\n",
       "      <td>0.08498</td>\n",
       "      <td>0.09293</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.06207</td>\n",
       "      <td>0.2710</td>\n",
       "      <td>0.7927</td>\n",
       "      <td>1.819</td>\n",
       "      <td>22.79</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.02017</td>\n",
       "      <td>0.030470</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>0.02769</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>14.77</td>\n",
       "      <td>20.50</td>\n",
       "      <td>97.67</td>\n",
       "      <td>677.3</td>\n",
       "      <td>0.1478</td>\n",
       "      <td>0.22560</td>\n",
       "      <td>0.30090</td>\n",
       "      <td>0.09722</td>\n",
       "      <td>0.3849</td>\n",
       "      <td>0.08633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
       "508       16.300         15.70  ...          0.2300                  0.07230\n",
       "524        9.847         15.68  ...          0.2502                  0.09209\n",
       "161       19.190         15.94  ...          0.2443                  0.06251\n",
       "330       16.030         15.51  ...          0.3019                  0.09124\n",
       "269       10.710         20.39  ...          0.2605                  0.08701\n",
       "..           ...           ...  ...             ...                      ...\n",
       "331       12.980         19.35  ...          0.3596                  0.09166\n",
       "429       12.720         17.67  ...          0.2165                  0.06025\n",
       "177       16.460         20.11  ...          0.3054                  0.09519\n",
       "500       15.040         16.74  ...          0.2177                  0.08549\n",
       "154       13.150         15.34  ...          0.3849                  0.08633\n",
       "\n",
       "[455 rows x 30 columns]"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>8.571</td>\n",
       "      <td>13.10</td>\n",
       "      <td>54.53</td>\n",
       "      <td>221.3</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.07632</td>\n",
       "      <td>0.025650</td>\n",
       "      <td>0.01510</td>\n",
       "      <td>0.1678</td>\n",
       "      <td>0.07126</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.6793</td>\n",
       "      <td>1.069</td>\n",
       "      <td>7.254</td>\n",
       "      <td>0.007897</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.018010</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>0.01592</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>9.473</td>\n",
       "      <td>18.45</td>\n",
       "      <td>63.30</td>\n",
       "      <td>275.6</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.22350</td>\n",
       "      <td>0.17540</td>\n",
       "      <td>0.08512</td>\n",
       "      <td>0.2983</td>\n",
       "      <td>0.10490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>13.560</td>\n",
       "      <td>13.90</td>\n",
       "      <td>88.59</td>\n",
       "      <td>561.3</td>\n",
       "      <td>0.10510</td>\n",
       "      <td>0.11920</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.04451</td>\n",
       "      <td>0.1962</td>\n",
       "      <td>0.06303</td>\n",
       "      <td>0.2569</td>\n",
       "      <td>0.4981</td>\n",
       "      <td>2.011</td>\n",
       "      <td>21.030</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.023140</td>\n",
       "      <td>0.025440</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>0.01842</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>14.980</td>\n",
       "      <td>17.13</td>\n",
       "      <td>101.10</td>\n",
       "      <td>686.6</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.26980</td>\n",
       "      <td>0.25770</td>\n",
       "      <td>0.09090</td>\n",
       "      <td>0.3065</td>\n",
       "      <td>0.08177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>11.310</td>\n",
       "      <td>19.04</td>\n",
       "      <td>71.80</td>\n",
       "      <td>394.1</td>\n",
       "      <td>0.08139</td>\n",
       "      <td>0.04701</td>\n",
       "      <td>0.037090</td>\n",
       "      <td>0.02230</td>\n",
       "      <td>0.1516</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.9429</td>\n",
       "      <td>1.831</td>\n",
       "      <td>18.150</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.020630</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.02183</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>12.330</td>\n",
       "      <td>23.84</td>\n",
       "      <td>78.00</td>\n",
       "      <td>466.7</td>\n",
       "      <td>0.12900</td>\n",
       "      <td>0.09148</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.06961</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.06641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>11.660</td>\n",
       "      <td>17.07</td>\n",
       "      <td>73.70</td>\n",
       "      <td>421.0</td>\n",
       "      <td>0.07561</td>\n",
       "      <td>0.03630</td>\n",
       "      <td>0.008306</td>\n",
       "      <td>0.01162</td>\n",
       "      <td>0.1671</td>\n",
       "      <td>0.05731</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.6724</td>\n",
       "      <td>2.225</td>\n",
       "      <td>26.030</td>\n",
       "      <td>0.006583</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.02216</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>13.280</td>\n",
       "      <td>19.74</td>\n",
       "      <td>83.61</td>\n",
       "      <td>542.5</td>\n",
       "      <td>0.09958</td>\n",
       "      <td>0.06476</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.04262</td>\n",
       "      <td>0.2731</td>\n",
       "      <td>0.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>8.597</td>\n",
       "      <td>18.60</td>\n",
       "      <td>54.09</td>\n",
       "      <td>221.2</td>\n",
       "      <td>0.10740</td>\n",
       "      <td>0.05847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2163</td>\n",
       "      <td>0.07359</td>\n",
       "      <td>0.3368</td>\n",
       "      <td>2.7770</td>\n",
       "      <td>2.222</td>\n",
       "      <td>17.810</td>\n",
       "      <td>0.020750</td>\n",
       "      <td>0.014030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06146</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>8.952</td>\n",
       "      <td>22.44</td>\n",
       "      <td>56.65</td>\n",
       "      <td>240.1</td>\n",
       "      <td>0.13470</td>\n",
       "      <td>0.07767</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.3142</td>\n",
       "      <td>0.08116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>12.200</td>\n",
       "      <td>15.21</td>\n",
       "      <td>78.01</td>\n",
       "      <td>457.9</td>\n",
       "      <td>0.08673</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.019940</td>\n",
       "      <td>0.01692</td>\n",
       "      <td>0.1638</td>\n",
       "      <td>0.06129</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.8073</td>\n",
       "      <td>1.959</td>\n",
       "      <td>19.010</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>0.014180</td>\n",
       "      <td>0.010510</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.01333</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>13.750</td>\n",
       "      <td>21.38</td>\n",
       "      <td>91.11</td>\n",
       "      <td>583.1</td>\n",
       "      <td>0.12560</td>\n",
       "      <td>0.19280</td>\n",
       "      <td>0.11670</td>\n",
       "      <td>0.05556</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>0.07961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>12.890</td>\n",
       "      <td>13.12</td>\n",
       "      <td>81.89</td>\n",
       "      <td>515.9</td>\n",
       "      <td>0.06955</td>\n",
       "      <td>0.03729</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.01171</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.05581</td>\n",
       "      <td>0.1532</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>1.115</td>\n",
       "      <td>12.680</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>0.016520</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.01619</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>13.620</td>\n",
       "      <td>15.54</td>\n",
       "      <td>87.40</td>\n",
       "      <td>577.0</td>\n",
       "      <td>0.09616</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.05366</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>0.06915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>9.742</td>\n",
       "      <td>15.67</td>\n",
       "      <td>61.50</td>\n",
       "      <td>289.9</td>\n",
       "      <td>0.09037</td>\n",
       "      <td>0.04689</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>0.01407</td>\n",
       "      <td>0.2081</td>\n",
       "      <td>0.06312</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>1.4090</td>\n",
       "      <td>1.750</td>\n",
       "      <td>16.390</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>0.01798</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>10.750</td>\n",
       "      <td>20.88</td>\n",
       "      <td>68.09</td>\n",
       "      <td>355.2</td>\n",
       "      <td>0.14670</td>\n",
       "      <td>0.09370</td>\n",
       "      <td>0.04043</td>\n",
       "      <td>0.05159</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.08175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>11.130</td>\n",
       "      <td>16.62</td>\n",
       "      <td>70.47</td>\n",
       "      <td>381.1</td>\n",
       "      <td>0.08151</td>\n",
       "      <td>0.03834</td>\n",
       "      <td>0.013690</td>\n",
       "      <td>0.01370</td>\n",
       "      <td>0.1511</td>\n",
       "      <td>0.06148</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.9671</td>\n",
       "      <td>0.968</td>\n",
       "      <td>9.704</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.02009</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>11.680</td>\n",
       "      <td>20.29</td>\n",
       "      <td>74.35</td>\n",
       "      <td>421.1</td>\n",
       "      <td>0.10300</td>\n",
       "      <td>0.06219</td>\n",
       "      <td>0.04580</td>\n",
       "      <td>0.04044</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>0.07083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>14.990</td>\n",
       "      <td>22.11</td>\n",
       "      <td>97.53</td>\n",
       "      <td>693.7</td>\n",
       "      <td>0.08515</td>\n",
       "      <td>0.10250</td>\n",
       "      <td>0.068590</td>\n",
       "      <td>0.03876</td>\n",
       "      <td>0.1944</td>\n",
       "      <td>0.05913</td>\n",
       "      <td>0.3186</td>\n",
       "      <td>1.3360</td>\n",
       "      <td>2.310</td>\n",
       "      <td>28.510</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.028080</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.011960</td>\n",
       "      <td>0.01906</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>16.760</td>\n",
       "      <td>31.55</td>\n",
       "      <td>110.20</td>\n",
       "      <td>867.1</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.33450</td>\n",
       "      <td>0.31140</td>\n",
       "      <td>0.13080</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.09251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
       "525        8.571         13.10  ...          0.2983                  0.10490\n",
       "221       13.560         13.90  ...          0.3065                  0.08177\n",
       "67        11.310         19.04  ...          0.2400                  0.06641\n",
       "350       11.660         17.07  ...          0.2731                  0.06825\n",
       "314        8.597         18.60  ...          0.3142                  0.08116\n",
       "..           ...           ...  ...             ...                      ...\n",
       "324       12.200         15.21  ...          0.2661                  0.07961\n",
       "287       12.890         13.12  ...          0.2309                  0.06915\n",
       "273        9.742         15.67  ...          0.2841                  0.08175\n",
       "428       11.130         16.62  ...          0.2383                  0.07083\n",
       "413       14.990         22.11  ...          0.3163                  0.09251\n",
       "\n",
       "[114 rows x 30 columns]"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Normalization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=StandardScaler()\n",
    "x_train=ss.fit_transform(x_train)\n",
    "x_test=ss.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60253271, -0.8501462 ,  0.51218608, ...,  0.33180835,\n",
       "        -0.92375978, -0.61970534],\n",
       "       [-1.24425787, -0.85495747, -1.22156067, ..., -0.76912058,\n",
       "        -0.59680452,  0.46608523],\n",
       "       [ 1.42962473, -0.79241096,  1.41024195, ...,  0.988426  ,\n",
       "        -0.69230135, -1.15683974],\n",
       "       ...,\n",
       "       [ 0.64832327,  0.21073893,  0.70343872, ...,  1.39177684,\n",
       "         0.29665738,  0.63616865],\n",
       "       [ 0.24193204, -0.59996014,  0.26397342, ..., -0.1981759 ,\n",
       "        -1.1228464 ,  0.10397215],\n",
       "       [-0.29896897, -0.93674907, -0.29398537, ..., -0.26977849,\n",
       "         1.58343676,  0.15005927]])"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.6094376 , -1.47561136, -1.57371498, ..., -0.45894691,\n",
       "         0.18173747,  1.16891379],\n",
       "       [-0.18163065, -1.28316054, -0.15761392, ..., -0.36858381,\n",
       "         0.31446188, -0.10012794],\n",
       "       [-0.82556042, -0.04666404, -0.85568605, ..., -0.70142643,\n",
       "        -0.76190074, -0.94286383],\n",
       "       ...,\n",
       "       [-1.27430793, -0.85736311, -1.28392566, ..., -0.98314667,\n",
       "        -0.04810237, -0.10122526],\n",
       "       [-0.8770748 , -0.62882776, -0.91098301, ..., -1.15746302,\n",
       "        -0.78941678, -0.7003578 ],\n",
       "       [ 0.22762249,  0.69186598,  0.21408142, ...,  0.25520295,\n",
       "         0.47308374,  0.48912879]])"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#converting data into tensor array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor=torch.from_numpy(x_train.astype(np.float32))\n",
    "x_test_tensor=torch.from_numpy(x_test.astype(np.float32))\n",
    "y_train_tensor=torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test_tensor=torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mymodel(nn.Module):\n",
    "    def __init__(self,new_features):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(new_features,3)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(3,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,features):\n",
    "        out=self.linear1(features)\n",
    "        out=self.relu(out)\n",
    "        out=self.linear2(out)\n",
    "        out=self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 30])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=y_train_tensor.view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 455])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2=y_train_tensor.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 1])"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "object=mymodel(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(object.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1,loss:0.6735861897468567\n",
      " 2,loss:0.6434317827224731\n",
      " 3,loss:0.6180444359779358\n",
      " 4,loss:0.5956094264984131\n",
      " 5,loss:0.5759220123291016\n",
      " 6,loss:0.5576412081718445\n",
      " 7,loss:0.5401744246482849\n",
      " 8,loss:0.5237489342689514\n",
      " 9,loss:0.5078815817832947\n",
      " 10,loss:0.4921848773956299\n",
      " 11,loss:0.47691094875335693\n",
      " 12,loss:0.462058424949646\n",
      " 13,loss:0.44757434725761414\n",
      " 14,loss:0.43354079127311707\n",
      " 15,loss:0.4199296832084656\n",
      " 16,loss:0.40663743019104004\n",
      " 17,loss:0.393907755613327\n",
      " 18,loss:0.3817327916622162\n",
      " 19,loss:0.3699793517589569\n",
      " 20,loss:0.3586239516735077\n",
      " 21,loss:0.3478708267211914\n",
      " 22,loss:0.33767056465148926\n",
      " 23,loss:0.32792219519615173\n",
      " 24,loss:0.31867045164108276\n",
      " 25,loss:0.3098921477794647\n",
      " 26,loss:0.3016022741794586\n",
      " 27,loss:0.29376521706581116\n",
      " 28,loss:0.28631865978240967\n",
      " 29,loss:0.2792689800262451\n",
      " 30,loss:0.27254149317741394\n",
      " 31,loss:0.2661373019218445\n",
      " 32,loss:0.26005974411964417\n",
      " 33,loss:0.25431838631629944\n",
      " 34,loss:0.2488728016614914\n",
      " 35,loss:0.24368558824062347\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    #forward pass\n",
    "    y_pred=object(x_train_tensor)\n",
    "    \n",
    "    #loss calculation\n",
    "    loss=loss_function(y_pred,y_train_tensor.view(-1,1))\n",
    "\n",
    "    #clean gradients or reset process\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #backward propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    #update step\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\" {epoch+1},loss:{loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 30])"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "mymodel                                  [455, 1]                  --\n",
       "├─Linear: 1-1                            [455, 3]                  93\n",
       "├─ReLU: 1-2                              [455, 3]                  --\n",
       "├─Linear: 1-3                            [455, 1]                  4\n",
       "├─Sigmoid: 1-4                           [455, 1]                  --\n",
       "==========================================================================================\n",
       "Total params: 97\n",
       "Trainable params: 97\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.04\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(object,input_size=(455,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=object(x_test_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred1>0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=y_test_tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred1==actual).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is : 0.9385964870452881\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred=object(x_test_tensor)\n",
    "    y_pred=(y_pred>0.5).float()\n",
    "    accuracy=(y_pred==y_test_tensor).float().mean()\n",
    "    print(f\"the accuracy is : {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset    ==> for initialization only\n",
    "\n",
    "#DataLoader ==> for sending data batchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creation of dummy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=make_classification(\n",
    "                        n_samples=20,   # no of datas or samples\n",
    "                        n_features=3,   # no of  columns\n",
    "                        n_informative=2,#no of informatives\n",
    "                        n_redundant=0,   #no reduntant in the datas\n",
    "                        n_classes=2,    #no classes in your label\n",
    "                        random_state=20 #for reproducability\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.92340582,  0.36302116,  0.52772847],\n",
       "       [ 0.35570789,  0.1384218 , -0.96221598],\n",
       "       [ 0.52777027,  0.42108815, -0.48245091],\n",
       "       [-1.57257964,  0.26749034,  0.15583822],\n",
       "       [ 1.34977647, -1.01834071,  0.09878861],\n",
       "       [-1.44465381, -0.35674894, -1.48238505],\n",
       "       [-0.1277233 ,  1.26247643,  1.3014294 ],\n",
       "       [-0.8463147 , -1.46683756, -0.85616422],\n",
       "       [ 0.73268863, -0.98972705, -1.48571961],\n",
       "       [-1.10201218,  1.10557442,  0.1140476 ],\n",
       "       [-0.33223102, -1.6432131 , -0.21966112],\n",
       "       [ 1.41891414, -1.13599391,  2.09466534],\n",
       "       [ 0.32626954,  1.47708907, -0.51314511],\n",
       "       [ 0.53801084,  1.55376789, -0.22832103],\n",
       "       [ 0.93928588, -1.07485961,  0.69100212],\n",
       "       [ 1.85971768,  0.63245719, -0.85477549],\n",
       "       [-0.48171213, -1.41999374, -0.82869277],\n",
       "       [-0.18163665,  1.71401726, -0.99480191],\n",
       "       [-1.86506731, -0.81756028,  1.50339816],\n",
       "       [ 1.17027916, -0.63548335,  0.35942899]])"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(x,dtype=torch.float32)\n",
    "y=torch.tensor(y,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Example of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mass(Dataset):\n",
    "    def __init__(self,features,labels):\n",
    "        self.features=features\n",
    "        self.labels=labels\n",
    "        \n",
    "    #dunder methods or special methods in python \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index],self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas=mass(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.5278,  0.4211, -0.4825]), tensor(1.))"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mas[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Example of DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader(mas,batch_size=2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BAFC5AD850>\n"
     ]
    }
   ],
   "source": [
    "print(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9234,  0.3630,  0.5277],\n",
      "        [ 0.3557,  0.1384, -0.9622]])\n",
      " tensor([1., 1.])\n",
      "--------------------------------------------------\n",
      "tensor([[ 0.5278,  0.4211, -0.4825],\n",
      "        [-1.5726,  0.2675,  0.1558]])\n",
      " tensor([1., 1.])\n",
      "--------------------------------------------------\n",
      "tensor([[ 1.3498, -1.0183,  0.0988],\n",
      "        [-1.4447, -0.3567, -1.4824]])\n",
      " tensor([0., 0.])\n",
      "--------------------------------------------------\n",
      "tensor([[-0.1277,  1.2625,  1.3014],\n",
      "        [-0.8463, -1.4668, -0.8562]])\n",
      " tensor([1., 0.])\n",
      "--------------------------------------------------\n",
      "tensor([[ 0.7327, -0.9897, -1.4857],\n",
      "        [-1.1020,  1.1056,  0.1140]])\n",
      " tensor([0., 1.])\n",
      "--------------------------------------------------\n",
      "tensor([[-0.3322, -1.6432, -0.2197],\n",
      "        [ 1.4189, -1.1360,  2.0947]])\n",
      " tensor([0., 0.])\n",
      "--------------------------------------------------\n",
      "tensor([[ 0.3263,  1.4771, -0.5131],\n",
      "        [ 0.5380,  1.5538, -0.2283]])\n",
      " tensor([1., 1.])\n",
      "--------------------------------------------------\n",
      "tensor([[ 0.9393, -1.0749,  0.6910],\n",
      "        [ 1.8597,  0.6325, -0.8548]])\n",
      " tensor([0., 1.])\n",
      "--------------------------------------------------\n",
      "tensor([[-0.4817, -1.4200, -0.8287],\n",
      "        [-0.1816,  1.7140, -0.9948]])\n",
      " tensor([0., 1.])\n",
      "--------------------------------------------------\n",
      "tensor([[-1.8651, -0.8176,  1.5034],\n",
      "        [ 1.1703, -0.6355,  0.3594]])\n",
      " tensor([0., 0.])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for batch_data,batch_labels in dataloader:\n",
    "    print(f\"{batch_data}\")\n",
    "    print(f\" {batch_labels}\")\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
